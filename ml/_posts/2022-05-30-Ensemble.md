---
layout: post
title: 앙상블
description: >
  앙상블 기법을 배웁니다.
sitemap: false
hide_last_modified: true
---

머신 러닝 기법 중 집단 지성을 활용하는 방법이 **앙상블 기법**입니다.  앙상블 기법은 다수의 모형의 결과를 종합하여 하나의 최종 결과를 도출하는 방법입니다. 

앙상블 기법에는 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking) 등이 있습니다. 

## Voting Classifier

데이터셋에 대하여 여러 알고리즘을 훈련시켰다고 합시다. 각 모델의 성능은 준수하다고 가정합니다. 이들을 이용하여 더 성능이 좋은 모델을 만들기 위한 가장 간단한 방법은 가장 많은 표를 받은 클래스를 예측하는 것입니다. 이와 같이 다수결의 원칙에 입각한 분류기를 하드 보팅 분류기(hard voting classifier)라고 합니다. 

개별 분류기가 랜덤한 추측보다 살짝 나을 뿐인 약한 학습자(weak learner)일지라도 충분한 수와 다양성이 보장된다면 앙상블은 높은 정확도를 얻는 강한 학습자(strong learner)일 수 있습니다. 

모든 분류기가 클래스의 확률을 추정할 수 있다면 가장 높은 클래스 확률의 평균을 이용하여 예측할 수도 있습니다. 이를 소프트 보팅(soft voting)이라고 합니다. 일반적으로 소프트 보팅이 하드 보팅보다 정확도가 높다고 알려져 있습니다. 

### scikit-learn

보팅 기법은 사이킷런의 `ensemble`에 있습니다. 

```python
class sklearn.ensemble.VotingClassifier(estimators, *, voting='hard', 
weights=None, n_jobs=None, flatten_transform=True, verbose=False)
```

```python
class sklearn.ensemble.VotingRegressor(estimators, *, weights=None, 
n_jobs=None, verbose=False)
```

### 분류 예시

로지스틱 회귀와 KNN, SVM의 세 가지 알고리즘을 사용하여 타이타닉 생존 분류에 대한 보팅 모델을 구성하도록 합니다. 

```python
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn.ensemble import VotingClassifier
```

먼저 **하드 보팅**을 적용합니다.

```python
lr_clf = LogisticRegression(C=0.1, max_iter=1000)
knn_clf = KNeighborsClassifier(n_neighbors=3)
rf_clf = RandomForestClassifier(max_features='sqrt', n_estimators=526)

vo_hard = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf), 
                                       ('RF', rf_clf)],
                           voting='hard')
vo_hard.fit(X_train, y_train)

print(f"트레인셋 정확도: {vo_hard.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {vo_hard.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9213
테스트셋 정확도: 0.7989
```

트레인셋의 정확도는 준수하게 나오는 반면 테스트셋의 정확도는 그에 못 미칩니다. 따라서 오버피팅이 발생함을 알 수 있습니다. 

각 알고리즘별 테스트셋의 정확도를 비교해보겠습니다. 

```python
for clf in (lr_clf, knn_clf, rf_clf, vo_hard):
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, pred))
```

```
LogisticRegression 0.8156424581005587
KNeighborsClassifier 0.7262569832402235
RandomForestClassifier 0.7988826815642458
VotingClassifier 0.7988826815642458
```

예상과는 다르게 보팅의 테스트셋 정확도가 가장 높지는 않습니다. 이는 트레인셋을 확인해보았을 때에도 마찬가지였습니다. 

이번에는 **소프트 보팅**을 적용해봅니다. 

```python
lr_clf = LogisticRegression(C=0.1, max_iter=1000)
knn_clf = KNeighborsClassifier(n_neighbors=3)
rf_clf = RandomForestClassifier(max_features='sqrt', n_estimators=526)

vo_soft = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf), 
                                       ('RF', rf_clf)],
                           voting='soft')
vo_soft.fit(X_train, y_train)

print(f"트레인셋 정확도: {vo_soft.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {vo_soft.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9326
테스트셋 정확도: 0.8156
```

여전히 오버피팅이 존재하나 하드 보팅에 비해 정도가 개선되어 보입니다. 

다시 한 번 각 알고리즘 별 테스트셋 정확도를 비교해보겠습니다.

```python
for clf in (lr_clf, knn_clf, rf_clf, vo_soft):
    clf.fit(X_train, y_train)
    pred = clf.predict(X_test)
    print(clf.__class__.__name__, np.round(accuracy_score(y_test, pred), 4)
```

```
LogisticRegression 0.8156
KNeighborsClassifier 0.7263
RandomForestClassifier 0.8212
VotingClassifier 0.8156
```

보팅의 정확도가 최고는 아니지만 이는 발생할 수 있는 결과입니다. 보다 일반적으로는 보팅 방식의 정확도가 더 높게 나올 것입니다. 

### 회귀 예시

선형회귀와 의사결정나무 모델을 이용하여 보팅 기법을 이용한 보스턴 집값 회귀모델을 구성하겠습니다. 회귀에서의 보팅은 분류와는 달리 소프트 보팅과 하드 보팅으로 나누어지지 않습니다.

```python
from sklearn.ensemble import VotingRegressor
```

```python
lr = LinearRegression()
dt = DecisionTreeRegressor(random_state=42)

voting = VotingRegressor(estimators = [('lr', lr), ('dt', dt)])
voting.fit(X_train_minmax, y_train)
pred_voting = voting.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, voting.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_voting)

print(f"Train R2: {voting.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {voting.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.9327
Test  R2: 0.8229
Train RMSE: 2.0691
Test  RMSE: 3.0080
```

오버피팅이 존재하지만 준수한 성능을 보이고 있습니다. 이제 개별 모델과 보팅 모델의 값을 비교해보겠습니다.

```python
for model in (lr, dt, voting):
    model.fit(X_train_minmax, y_train)
    pred = model.predict(X_test_minmax)
    print(model.__class__.__name__, np.round(r2_score(y_test, pred),4))
```

```
LinearRegression 0.7111
DecisionTreeRegressor 0.7935
VotingRegressor 0.8229
```

회귀모형에서는 개별 모델보다 보팅의 성능이 우수하게 나타나고 있습니다. 이는 평균한 값을 통하여 잔차가 감소된 결과라고 해석해 볼 수 있습니다. 

## Bagging and Pasting

보팅이 다양한 알고리즘을 사용하였다면 같은 알고리즘을 무작위로 샘플링한 트레인셋에 적용할 수도 있습니다. 

랜덤 복원 추출을 통하여 같은 크기의 표본을 추출하는 것을 부트스트랩 샘플링이라고 합니다. 부트스트랩 샘플링을 사용한 훈련데이터에 같은 알고리즘을 병렬적으로 학습시키는 기법을 배깅(Bagging, Bootstrap Aggregating)이라고 합니다. 복원 추출 대신 비복원 추출을 할 경우에는 패이스팅(Pasting)이라고 합니다. 

모든 샘플에 대한 학습이 끝난 후 분류에서는 통계적 최빈값, 회귀에서는 평균을 사용하여 결과를 종합합니다. 원본 훈련셋에 대하여 훈련시켰을 때보다 편향성을 갖게 되지만 이러한 종합 과정은 편향성과 분산 모두를 줄여줍니다. 일반적으로 앙상블 기법은 원본 훈련셋에 한 번 훈련시킨 모델과 비슷한 편향성을 갖지만 분산은 더 작은 경향이 있습니다. 

배깅은 더 다양한 훈련셋의 부분집합을 형성 가능하므로 패이스팅보다는 더 높은 편향성을 갖게 됩니다. 그러나 이는 모델이 덜 상관되어 앙상블의 분산이 줄어듦을 의미합니다. 전반적으로 배깅이 더 나은 모델이라고 생각됩니다. 

배깅은 데이터 사이즈가 작거나 결측값이 있는 경우 유리한 방법입니다.

### scikit-learn

배깅은 사이킷런의 `ensemble`에 있습니다.

```python
class sklearn.ensemble.BaggingClassifier(base_estimator=None, 
n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, 
bootstrap_features=False, oob_score=False, warm_start=False, 
n_jobs=None, random_state=None, verbose=0)
```

```python
class sklearn.ensemble.BaggingRegressor(base_estimator=None, 
n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, 
bootstrap_features=False, oob_score=False, warm_start=False, 
n_jobs=None, random_state=None, verbose=0)
```

### 분류 예시

```python
from sklearn.ensemble import BaggingClassifier
```

```python
bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)
bag_clf.fit(X_train, y_train)

print(f"트레인셋 정확도: {bag_clf.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {bag_clf.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9565
테스트셋 정확도: 0.7709
```

배깅에서도 오버피팅이 발생함을 확인할 수 있습니다.

### 회귀 예시

```python
from sklearn.ensemble import BaggingRegressor
```

```python
bagging = BaggingRegressor(base_estimator=KNeighborsRegressor(),
                           n_estimators=10, random_state=42)
bagging.fit(X_train_minmax, y_train)
pred_bagging = bagging.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, bagging.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_bagging)

print(f"Train R2: {bagging.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {bagging.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.8895
Test R2: 0.8318
Train RMSE: 2.6518
Test RMSE: 2.9312
```

KNN을 단독으로 사용하였을 때의 트레인셋과 테스트셋의 $R^2$ 값은 각각 0.8849와 0.8213이었습니다. 배깅에서의 결과가 KNN을 단독으로 사용하였을 때보다 개선되었음을 확인할 수 있었습니다. 

## Out-of-Bag Evaluation

배깅을 사용할 경우, 어떤 표본은 여러번 샘플링 될 것입니다. 기본적으로 배깅 분류기는 m 개의 훈련 표본을 복원추출합니다. 이는 각 모델에서 오직 훈련 표본의 63%만이 훈련된다는 것을 의미합니다. 나머지 훈련되지 않는 37%는 아웃오브백(out-of-bag, oob) 표본이라고 하며 훈련에 사용되지 않습니다. 따라서 따로 검증셋을 설정하지 않고서 oob 표본에 대한 평가를 시행하여 앙상블의 성능을 평가해볼 수도 있습니다. 

## Random Patches and Random Subspaces

배깅 분류기는 또한 피쳐 샘플링을 지원합니다. `max_features`와 `bootstrap_features` 하이퍼파라미터를 통해 표본을 샘플링하는 대신 피쳐들의 무작위 부분 집합에 대한 학습을 진행하게 됩니다. 이는 고차원의 입력 데이터를 다룰 때 유용합니다. 

표본관 피쳐 모두를 부트스트랩하는 것을 Random Patches 기법이라고 하며 피쳐만 샘플링하는 기법을 Random Subspaces 기법이라고 합니다. 피쳐를 샘플링하는 것은 더 다양한 모델을 가능하게 하여 약간의 편향성을 대가로 ㄴ더 낮은 분산을 가능하게 합니다. 
