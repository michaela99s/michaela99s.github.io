---
layout: post
title: 스태킹
description: >
  스태킹 기법을 살펴봅니다.
sitemap: false
hide_last_modified: true
---

스태킹(Stacked Generalization)은 여러 학습기에서 예측한 예측값으로 다시 학습 데이터를 만들어 일반화된 최종 모델을 구성하는 방법입니다. 다른 앙상블 기법과는 달리 스태킹은 데이터셋이 아닌 데이터셋을 통해 예측한 값들로 다시 예측을 한다는 차이점을 갖습니다. 

# Stacking

## scikit-learn

스태킹은 사이킷런의 `ensemble`에 있습니다.

```python
class sklearn.ensemble.StackingClassifier(estimators, 
final_estimator=None, *, cv=None, stack_method='auto', 
n_jobs=None, passthrough=False, verbose=0)
```

```python
class sklearn.ensemble.StackingRegressor(estimators, 
final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, 
verbose=0)
```

## 분류 예시

타이타닉 생존여부에 대해 KNN과 랜덤포레스트 두 가지를 개별 모델로 설정하여 스태킹해보도록 하겠습니다. 마지막 최종 모델로는 로지스틱회귀모델을 사용합니다. 

```python
from sklearn.ensemble import StackingClassifier
```

```python
knn_clf = KNeighborsClassifier(n_neighbors=3)
rf_clf = RandomForestClassifier(max_features='sqrt', n_estimators=526)

estimators = [('KNN', knn_clf), ('RF', rf_clf)]
model = StackingClassifier(estimators = estimators, 
                           final_estimator=LogisticRegression(max_iter=1000))
model.fit(X_train, y_train)

print(f"트레인셋 정확도: {model.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {model.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9803
테스트셋 정확도: 0.8212
```

이번에는 순서를 바꾸어 랜덤포레스트 다음에 KNN을 쌓도록 하겠습니다.

```python
knn_clf = KNeighborsClassifier(n_neighbors=3)
rf_clf = RandomForestClassifier(max_features='sqrt', n_estimators=526)

estimators = [('RF', rf_clf), ('KNN', knn_clf)]
model = StackingClassifier(estimators = estimators, 
                           final_estimator=LogisticRegression(max_iter=1000))
model.fit(X_train, y_train)

print(f"트레인셋 정확도: {model.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {model.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9803
테스트셋 정확도: 0.8101
```

## 회귀 예시

보스턴 집값 데이터셋을 스태킹 기법으로 예측합니다. 선형회귀와 KNN을 개별 모델로 하고 최종 예측 모델로는 랜덤 포레스트를 사용하겠습니다.

```python
from sklearn.ensemble import StackingRegressor
```

먼저 선형회귀 - KNN 순으로 쌓아보겠습니다.

```python
estimators = [('lr', LinearRegression()), 
              ('knn', KNeighborsRegressor(n_neighbors=5))]
model = StackingRegressor(estimators = estimators, 
                        final_estimator = RandomForestRegressor(random_state=42))
model.fit(X_train_minmax, y_train)
pred_model = model.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, model.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_model)

print(f"Train R2: {model.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {model.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.8897
Test  R2: 0.7839
Train RMSE: 2.6497
Test  RMSE: 3.3226
```

이번에는 KNN - 선형회귀 순으로 쌓아보겠습니다.

```python
estimators = [('knn', KNeighborsRegressor(n_neighbors=5)),
              ('lr', LinearRegression())]
model = StackingRegressor(estimators = estimators, 
                        final_estimator = RandomForestRegressor(random_state=42))
model.fit(X_train_minmax, y_train)
pred_model = model.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, model.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_model)

print(f"Train R2: {model.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {model.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```

```

## 종합 정리

이와 같이 스태킹은 모델을 어떻게 쌓는가에 따라서 결과가 달라집니다. 어떤 모델을 사용하는지, 그 순서는 어떻게 정하는지에 따라 더 나은 모델을 얻을 수 있습니다. 

스태킹은 부스팅과 마찬가지로 보다 강한 모델을 만들고자 하는 방법입니다. 그에 따라 오버피팅이 발생한다는 문제가 있습니다. 
