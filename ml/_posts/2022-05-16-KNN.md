---
layout: post
title: k-최근접이웃
description: >
  KNN 모델을 알아봅니다.
sitemap: false
hide_last_modified: true
---

**근주자적**이라는 사자성어가 있습니다. "붉은 색을 가까이하면 붉어진다"는 의미로 주변환경의 중요성을 나타냅니다. 이제부터 살펴볼 KNN(K - Nearest Neighbors: K 최근접이웃) 알고리즘도 이와 마찬가지로 주변 데이터의 레이블을 참조하여 분석을 진행합니다. 

KNN 알고리즘을 요약하면, 예측하고자 하는 데이터에 가장 가까운 k 개의 샘플을 선택하여 다수결의 원칙에 의하여 해당 데이터의 클래스를 정하는 것입니다. 회귀의 경우에는 가까운 k 개의 샘플의 평균으로 해당 샘플 데이터의 타겟값을 정하게 됩니다. 

# KNN

## scikit-learn

KNN은 scikit learn의 neighbors 안에 있습니다. 

```python
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, 
weights='uniform', algorithm='auto', leaf_size=30, p=2, 
metric='minkowski', metric_params=None, n_jobs=None)
```

```python
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, 
weights='uniform', algorithm='auto', leaf_size=30, p=2, 
metric='minkowski', metric_params=None, n_jobs=None)
```

- **n-neighbors**: 이웃의 수(K)를 정합니다. 기본값은 5이며 일반적으로 3~10 사이의 값으로 설정합니다. 또는, 데이터 수의 제곱근 값으로 설정하기도 하지만 다양한 K 값을 탐색하여야 합니다. K 값이 작을수록 정교한 분석이 가능하지만 오버피팅의 우려가 있습니다.

- weights: {'uniform', 'distance'} or callable
  
  - 'uniform': 균등 가중치. 모든 점의 가중치가 동등하다고 여겨집니다.
  
  - 'distance': 점의 가중치가 거리의 역수로 주어집니다. 가까운 점일수록 영향이 더 커집니다.

- metric: 거리를 측정하는 방법입니다. (177)
  
  - 연속형 - 수학적 거리: "euclidean", "manhattan", "minkowski"
  
  - 연속형 - 통계적 거리: "mahalanobis"
  
  - 명목형 : "matching", "jaccard"
  
  - 

## 분류 예시

타이타닉 데이터셋에 KNN 알고리즘을 적용해보겠습니다. 

```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

print(f"트레인셋 정확도: {knn.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {knn.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.7963
테스트셋 정확도: 0.7430
```

모델의 성능이 매우 좋다고 보기는 어렵습니다. 따라서 하이퍼파라미터를 조정해보겠습니다. 

### Hyperparameter Tuning

#### Grid Search

이웃의 수인 K의 값을 조정하도록 하겠습니다. 약간의 언더피팅이 우려되므로 하이퍼파라미터 튜닝의 결과 최적 K는 5보다 작은 값을 갖게 될 것이라고 예상해볼 수 있습니다.

```python
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11]}

grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Test Score: {grid_search.score(X_test, y_test):.4f}")
```

```
Best Parameter: {'n_neighbors': 3}
Best Score: 0.7008
Test Score: 0.7263
```

예상대로 최적 K 값은 5보다 작은 3이 되었습니다. 

#### Random Search

랜덤서치를 사용하여 1부터 20까지의 범위에서 최적 K 값을 탐색해보았을 때에도 결과는 그리드서치와 마찬가지로 K 값은 3에서 최적이라는 결과를 얻을 수 있었습니다.

```python
param_distribs = {'n_neighbors': randint(low=1, high=20)}

random_search = RandomizedSearchCV(KNeighborsClassifier(),
                                   param_distributions=param_distribs, n_iter=20, cv=5)
random_search.fit(X_train, y_train)

print(f"Best Parameter: {random_search.best_params_}")
print(f"Best Score: {random_search.best_score_:.4f}")
print(f"Test Score: {random_search.score(X_test, y_test):.4f}")
```

```
Best Parameter: {'n_neighbors': 3}
Best Score: 0.7008
Test Score: 0.7263
```

## 회귀 예시

보스턴 집값을 KNN 알고리즘을 적용하여 예측해봅니다.

```python
from sklearn.neighbors import KNeighborsRegressor
```

```python
knn = KNeighborsRegressor()
knn.fit(X_train_minmax, y_train)
pred_knn = knn.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, knn.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_knn)

print(f"Train R2: {knn.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {knn.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.8849
Test  R2: 0.8213
Train RMSE: 2.7068
Test  RMSE: 3.0217
```

KNN 회귀 결과 준수한 성능을 보이고 있으며 심한 오버피팅이 나타나지도 않고 있습니다. 

### Hyperparameter Tuning

#### Grid Search

```python
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11]}
grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)
grid_search.fit(X_train_minmax, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Train Score: {grid_search.score(X_train_minmax, y_train):.4f}")
print(f"Test  Score: {grid_search.score(X_test_minmax, y_test):.4f}")
```

```
Best Parameter: {'n_neighbors': 3}
Best Score: 0.8233
Train Score: 0.9171
Test  Score: 0.8061
```

그리드 서치의 결과 K 값은 3으로 기본값인 5보다 작았습니다. 

#### Random Search

```python
param_distribs = {'n_neighbors': randint(low=1, high=20)}
random_search = RandomizedSearchCV(KNeighborsRegressor(),
                                   param_distributions=param_distribs, 
                                   n_iter=20, cv=5)
random_search.fit(X_train_minmax, y_train)

print(f"Best Parameter: {random_search.best_params_}")
print(f"Best Score: {random_search.best_score_:.4f}")
print(f"Train Score: {random_search.score(X_train_minmax, y_train):.4f}")
print(f"Test  Score: {random_search.score(X_test_minmax, y_test):.4f}")
```

```
Best Parameter: {'n_neighbors': 3}
Best Score: 0.8233
Train Score: 0.9171
Test  Score: 0.8061
```

랜덤서치의 결과 역시 그리드서치와 마찬가지로 최적의 K로 3이 나왔습니다. 

그러나 하이퍼파라미터를 조정한 후의 $R^2$값은 기존 모델에 비하여 테스트셋의 성능이 안 좋을뿐만 아니라 오버피팅이 심화되었습니다. 보통 하이퍼파라미터 조정을 통해서 모델의 오버피팅을 개선할 수 있다고 알려져 있습니다. 하지만 때로는 정반대의 결과가 나올 수도 있습니다. 

먼저, 하이퍼파라미터 조정에는 교차검증을 사용합니다. 그리고 결과의 평균으로 모델을 결정합니다. 따라서 데이터셋의 구성에 따라 다른 결과를 불러올 수가 있습니다. 또한, 평가지표로서 $R^2$가 항상 모델의 성능을 정확하게 표현하는 것은 아닙니다. 따라서 최종 모델을 선택할 때에는 다양한 평가지표를 확인하고 비교해볼 필요가 있습니다.





## 종합정리

KNN은 심플하면서 직관적이라는 장점이 있습니다. 그러나 KNN은 모델을 기반으로 한 알고리즘이 아닌 **케이스 기반**의 알고리즘입니다. 따라서 어떠한 데이터가 입력되느냐에 따라 결과와 성능에서 큰 차이를 나타낼 수도 있습니다. 
