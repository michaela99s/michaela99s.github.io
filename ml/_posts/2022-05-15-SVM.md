---
layout: post
title: 서포트 벡터 머신
description: >
  SVM에 대하여 배웁니다.
sitemap: false
hide_last_modified: true
---

Support Vector Machine은 매우 강력하고 유용한 알고리즘으로 선형 또는 비선형의 분류, 회귀뿐만 아니라 이상치 탐지까지 가능합니다. SVM은 특히 중간크기 이하의 데이터셋의 복잡한 분류 문제에 유용합니다. 

# 서포트 벡터 머신 Support Vector Machine

## SVM의 원리

### SVM의 구성요소

SVM의 구성요소로는 결정경계, 초평면, 마진, 서포트벡터, 슬랙변수가 있습니다. 피쳐의 수가 N인 분류 문제가 있다고 합시다.

- 결정경계(Decision Boundary): 데이터 분류의 기준이 되는 경계

- 초평면(Hyperplane): 데이터를 분류하는 (N-1) 차원의 결정경계, 선형 SVM에서의 결정경계, 회귀에서는 예측 선

- 마진(Margin): 결정경계에서 서포트벡터까지의 거리

- 슬랙변수: 완벽한 분리가 불가능할 때 허용된 오차를 위한 변수

SVM의 목표는 N 차원의 공간에서 데이터를 분류하는 **최대 마진**을 갖는 결정경계를 찾는 것입니다. 

### SVM의 종류

SVM은 오분류의 허용여부에 따라 하드(hard) 마진과 소프트(soft) 마진으로 구분됩니다. 

하드 마진은 오분류를 허용하지 않습니다. 이로 인해, 노이즈가 많은 데이터셋에서는 최적 결정경계를 구하지 못하는 경우가 발생합니다. 

반면, 소프트 마진은 오분류를 허용하되 벌점을 부과합니다. 잘못 분류된 데이터는 본래의 카테고리로 비용을 들여 이동시킵니다. 

## scikit-learn

SVM 분류와 회귀는 사이킷런의 `svm`에 각각 `SVC`와 `SVR`로 있습니다.

```python
class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', 
coef0=0.0, shrinking=True, probability=False, tol=0.001, 
cache_size=200, class_weight=None, verbose=False, max_iter=- 1, 
decision_function_shape='ovr', break_ties=False, random_state=None)
```

```python
class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', 
coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, 
cache_size=200, verbose=False, max_iter=- 1)
```

- C: 규제 변수. 잘못된 분류를 생성할 때 부여하는 페널티를 의미합니다. 회귀에서는 l2 규제의 정도를 나타냅니다. 규제의 정도는 C 값에 역비례하고 큰 값 설정시 오버피팅이 발생합니다. 작은 값 설정시(높은 수준의 규제를 할 경우) 언더피팅이 발생하며 모델이 단순해집니다.

- kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}
  알고리즘에 사용될 커널을 지정합니다. 선형 분류가 불가능한 데이터를 처리하기 위하여 데이터의 차원을 증가시켜 하나의 초평면을 분리가 가능하도록 커널 함수를 바꾸어줄 수 있습니다. 이처럼 저차원 공간을 고차원 공간으로 매핑하도록 커널을 바꾸어주는 것을 **커널 트릭**(kernel trick)이라고 합니다. 

- gamma: 'rbf', 'poly', 'sigmoid'에서의 커널 계수. 감마 값이 커질수록 결정경계가 더 복잡해지며 과도한 연산을 초래할 수 있지만 인식률이 향상됩니다. 데이터 양과 특성에 적절한 값 설정하여야 합니다. 

## 분류 예시

타이타닉 생존 분류에 SVM을 적용해보겠습니다.

```python
from sklearn.svm import SVC
```

```python
svc = SVC()
svc.fit(X_train, y_train)

print(f"트레인셋 정확도: {svc.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {svc.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.6699
테스트셋 정확도: 0.6592
```

트레인셋과 테스트셋 모두 정확도가 낮게 나옵니다. 이는 과소적합(underfitting)됨을 의미합니다. 따라서 하이퍼파라미터의 조정이 필요해 보입니다.

### Hyperparameter tuning

하이퍼파라미터 중 `kernel`, `C`, `gamma` 값을 조정합니다.

#### Grid Search

```python
param_grid = {'kernel': ['rbf', 'linear'], 
               'C': [0.001, 0.01, 0.1, 1, 10, 100],
               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}

grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Test Score: {grid_search.score(X_test, y_test):.4f}")
```

```
Best Parameter: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}
Best Score: 0.7949
Test Score: 0.7933
```

기본 모델보다 성능이 많이 개선되었습니다. 

#### Random Search

```python
param_distribs = {'kernel': ['rbf'], 
                  'C': randint(low=0.001, high=100),
                  'gamma': randint(low=0.001, high=100)}

random_search = RandomizedSearchCV(SVC(),
                                   param_distributions=param_distribs, n_iter=10, cv=5)
random_search.fit(X_train, y_train)

print(f"Best Parameter: {random_search.best_params_}")
print(f"Best Score: {random_search.best_score_:.4f}")
print(f"Test Score: {random_search.score(X_test, y_test):.4f}")
```

```
Best Parameter: {'C': 25, 'gamma': 90, 'kernel': 'rbf'}
Best Score: 0.6461
Test Score: 0.6089
```

랜덤 서치의 성능은 그렇게 좋아보이지 않습니다. 그리드 서치에서는 C의 값이 100으로 지정된 반면, 감마값이 매우 작은 값을 갖게 되었습니다. 그러나 랜덤 서치에서는 C의 값은 25, 감마값은 90으로 다소 상반된 하이퍼파라미터를 설정하였습니다. 두 방식의 성능차이는 여기에서 기인합니다. 

## 회귀 예시

SVM으로 보스턴 집값을 예측해봅니다.

```python
from sklearn.svm import SVR
```

```python
svr = SVR()
svr.fit(X_train_minmax, y_train)
pred_svr = svr.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, svr.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_svr)

print(f"Train R2: {svr.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {svr.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.8052
Test  R2: 0.7800
Train RMSE: 3.5204
Test  RMSE: 3.3526
```

오버피팅 현상은 나타나지 않은 것처럼 보입니다. 그러나 아직 모델의 성능을 개선할 여지가 있어 보입니다.

### Hyperparameter Tuning

`C`와 `gamma`값을 조정하여 모델 성능을 개선해봅시다.

#### Grid Search

```python
param_grid = {'kernel': ['poly'], 'C': [0.01, 0.1, 1, 10],
              'gamma': [0.01, 0.1, 1, 10]}
grid_search = GridSearchCV(SVR(), param_grid, cv=5)
grid_search.fit(X_train_minmax, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Train Score: {grid_search.score(X_train_minmax, y_train):.4f}")
print(f"Test  Score: {grid_search.score(X_test_minmax, y_test):.4f}")
```

```
Best Parameter: {'C': 0.1, 'gamma': 10, 'kernel': 'poly'}
Best Score: 0.8335
Train Score: 0.8625
Test  Score: 0.8261
```

#### Random Search

```python
param_distribs = {'kernel': ['poly'], 'C': randint(low=1, high=10),
                  'gamma': randint(low=1, high=10)}
random_search = RandomizedSearchCV(SVR(),
                                   param_distributions=param_distribs, 
                                   n_iter=20, cv=5)
random_search.fit(X_train_minmax, y_train)

print(f"Best Parameter: {random_search.best_params_}")
print(f"Best Score: {random_search.best_score_:.4f}")
print(f"Train Score: {random_search.score(X_train_minmax, y_train):.4f}")
print(f"Test  Score: {random_search.score(X_test_minmax, y_test):.4f}")
```

```
Best Parameter: {'C': 2, 'gamma': 4, 'kernel': 'poly'}
Best Score: 0.8338
Train Score: 0.8648
Test  Score: 0.8265
```

그리드 서치에서는 규제의 강도가 기본값보다 세진데 반해, 랜덤 서치에서는 규제의 강도가 약해졌습니다. 그럼에도 불구하고 두 모델의 성능은 비슷하게 나타나고 있습니다. 이를 해석하기 위해서는 하이퍼파라미터에 대한 깊은 이해가 필요해 보입니다. 

## 종합 정리

SVM은 고차원 공간에서 효과적이며 표본의 수보다 차원의 수가 큰 경우에도 효과적입니다.  결정 함수에서 훈련 되는 데이터의 일부분(서포트 벡터)을 사용하여 메모리 효율적이고 다른 커널 함수들을 사용할 수 있어 다재다능(유연)하다는 장점이 있습니다.

한편, SVM은 크기가 큰 데이터셋에서는 잘 작동하지 않으며 학습속도도 느립니다. 노이즈가 많은 경우 즉, 타겟이 겹칠 경우 잘 작동하지 않습니다. 따라서 표본의 수보다 피쳐의 수가 월등히 크면 적정한 커널함수와 규제를 사용하는 것이 오버피팅을 피하는 것에 있어서 매우 중요합니다. 또한 확률 추정치를 계산해주지 않기 때문에 교차 검증을 사용하여야 합니다.
