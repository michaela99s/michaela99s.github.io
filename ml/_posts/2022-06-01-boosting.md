---
layout: post
title: 부스팅
description: >
  부스팅 기법을 살펴봅니다.
sitemap: false
hide_last_modified: true
---

예측력이 약한 모형(weak learner)들을 결합하여 예측력이 강한 모형(strong learner)을 만드는 앙상블 기법을 **부스팅**(Boosting)이라고 합니다. 순차적으로 모형을 학습하며 이전 모형의 오류를 수정해 나가는 것이 기본 아이디어입니다. 부스팅은 높은 정확도를 보이나 오버피팅 가능성이 높고 이상치에 취약합니다. 

약한 모형을 순차적으로 학습시켜 잘못 예측한 데이터에 더 큰 가중치를 부여하여 오류를 개선해 나가는 앙상블 모형입니다. 이 가중치 조정 방법에 따라 다양한 부스팅 알고리즘으로 나뉘어 집니다.  

## AdaBoost(Adaptive Boosting)

AdaBoost는 모형을 순차적으로 학습하며, 먼저 학습한 모형에서 잘못 예측한 데이터에 더 좊은 가중치를 부여하여 다음 모형에서 해당 데이터를 더 잘 예측할 수 있도록 합니다. 가중치는 모형을 거쳐가며 업데이트 되어 최종적으로 더 나은 예측 결과를 가능하게 합니다.

### scikit-learn

AdaBoosting은 사이킷런의 `ensemble`에 있습니다.

```python
class sklearn.ensemble.AdaBoostClassifier(base_estimator=None, *, 
n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', 
random_state=None)
```

```python
class sklearn.ensemble.AdaBoostRegressor(base_estimator=None, *, 
n_estimators=50, learning_rate=1.0, loss='linear', 
random_state=None)
```

- base_estimator: 어떤 모델을 기준으로 부스트 앙상블이 생성될 것인가를 정합니다. 기본값은 `None`이며 이때 기저 모델은 분류의 경우 `max_depth=1`, 회귀의 경우 `max_depth=3`인 의사결정나무입니다.  

- n_estimator: 부스팅이 끝날 때의 최대 모델의 수입니다. 

### 분류 예시

타이타닉 생존여부를 AdaBoosting으로 분류해봅시다.

```python
from sklearn.ensemble import AdaBoostClassifier
```

```python
ada_clf = AdaBoostClassifier()
ada_clf.fit(X_train, y_train)

print(f"트레인셋 정확도: {ada_clf.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {ada_clf.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.8329
테스트셋 정확도: 0.8045
```

### 회귀 예시

보스턴 집값을 AdaBoosting으로 예측해봅니다.

```python
from sklearn.ensemble import AdaBoostRegressor
```

```python
ada = AdaBoostRegressor(random_state=42)
ada.fit(X_train_minmax, y_train)
pred_ada = ada.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, ada.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_ada)

print(f"Train R2: {ada.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {ada.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.8938
Test  R2: 0.8502
Train RMSE: 2.5999
Test  RMSE: 2.7667
```

## GBM: Gradient Boosting Model

Gradient Boosting 알고리즘은 경사하강법을 사용하여 오차를 최소화하는 방향으로 가중치를 조정합니다. Gradient Boosting은 대표적인 탐욕 알고리즘(greedy algorithm)으로 문제를 해결하는 과정에서 매순간 가능한 모든 선택지를 탐색하여 최선의 답을 선택합니다. 따라서 과적합될 확률이 높고 학습시간이 길다는 단점이 있습니다. 

### scikit-learn

GBM은 사이킷런의 `ensemble`에 있습니다.

```python
class sklearn.ensemble.GradientBoostingClassifier(*, loss='log_loss', 
learning_rate=0.1, n_estimators=100, subsample=1.0, 
criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, 
min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, 
init=None, random_state=None, max_features=None, verbose=0, 
max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, 
n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)
```

```python
class sklearn.ensemble.GradientBoostingRegressor(*, 
loss='squared_error', learning_rate=0.1, n_estimators=100, 
subsample=1.0, criterion='friedman_mse', min_samples_split=2, 
min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, 
min_impurity_decrease=0.0, init=None, random_state=None, 
max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, 
warm_start=False, validation_fraction=0.1, n_iter_no_change=None, 
tol=0.0001, ccp_alpha=0.0)
```

- loss: {'log_loss', 'deviance', 'exponential'} {'squared_error', 'absolute_error', 'huber', 'quantile'}
  최적화될 손실함수

- learning_rate: 학습률. 학습률과 `n_estimators` 사이에는 상충관계가 있습니다. 

- n_estimators:  부스팅 단계의 수입니다. 보통 이 값이 클수록 더 나은 성능을 보입니다. 

### 분류 예시

타이타닉 데이터셋에 GBM을 적용해봅니다.

```python
from sklearn.ensemble import GradientBoostingClassifier
```

```python
gb_clf = GradientBoostingClassifier()
gb_clf.fit(X_train, y_train)

print(f"트레인셋 정확도: {gb_clf.score(X_train, y_train):.4f}")
print(f"테스트셋 정확도: {gb_clf.score(X_test, y_test):.4f}")
```

```
트레인셋 정확도: 0.9045
테스트셋 정확도: 0.8156
```

#### Hyperparameter Tuning

```python
param_grid = {'n_estimators': [100, 500],
              'learning_rate': [0.05, 0.1]}

grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, 
                           cv=5, verbose=1)
grid_search.fit(X_train, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Test Score: {grid_search.score(X_test, y_test):.4f}")
```

```
Fitting 5 folds for each of 4 candidates, totalling 20 fits
Best Parameter: {'learning_rate': 0.05, 'n_estimators': 100}
Best Score: 0.8244
Test Score: 0.8045
```

### 회귀 예시

보스턴 집값 데이터셋에 GBM을 적용해봅니다.

```python
from sklearn.ensemble import GradientBoostingRegressor
```

```python
gbm = GradientBoostingRegressor(random_state=42)
gbm.fit(X_train_minmax, y_train)
pred_gbm = gbm.predict(X_test_minmax)

MSE_train = mean_squared_error(y_train, gbm.predict(X_train_minmax))
MSE_test = mean_squared_error(y_test, pred_gbm)

print(f"Train R2: {gbm.score(X_train_minmax, y_train):.4f}")
print(f"Test  R2: {gbm.score(X_test_minmax, y_test):.4f}")
print(f"Train RMSE: {np.sqrt(MSE_train):.4f}")
print(f"Test  RMSE: {np.sqrt(MSE_test):.4f}")
```

```
Train R2: 0.9602
Test  R2: 0.8712
Train RMSE: 1.5910
Test  RMSE: 2.5651
```

#### Hyperparameter Tuning

```python
param_grid = {'n_estimators': [100, 200, 300, 400, 500],
              'learning_rate': [0.05, 0.1]}
grid_search = GridSearchCV(GradientBoostingRegressor(), param_grid, cv=5)
grid_search.fit(X_train_minmax, y_train)

print(f"Best Parameter: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_:.4f}")
print(f"Train Score: {grid_search.score(X_train_minmax, y_train):.4f}")
print(f"Test  Score: {grid_search.score(X_test_minmax, y_test):.4f}")
```

```
Best Parameter: {'learning_rate': 0.05, 'n_estimators': 500}
Best Score: 0.8711
Train Score: 0.9839
Test  Score: 0.8563
```

하이퍼파라미터의 조정 결과 오히려 과적합이 심화되었습니다. 

## 정리

부스팅기법은 구현이 쉽고 편향을 줄일 수 있다는 장점이 있습니다. 그러나 이상치에 취약하다는 단점을 가지고 있습니다. 

Adaboosting과 Gradient Boosting은 과적합이 발생한다는 단점이 있습니다. 이를 보완하기 위하여 다양한 부스팅 방법이 도입되었습니다. 

**XGBoost**는 시스템 최적화와 알고리즘 고도화를 통하여 GBM의 단점을 보완하고자 합니다. 시스템 최적화의 측면에서는 병렬화, 가지치기를 적용하고 알고리즘 고도화 측면에서는 정규화, 결측치처리, 교차검증을 적용합니다. XGboost를 사용하기 위해서는 `xgboost` 라이브러리를 임포트하여야 합니다.

```python
class xgboost.XGBRegressor(*, objective='reg:squarederror', **kwargs)
```

```python
class xgboost.XGBClassifier(*, objective='binary:logistic', 
use_label_encoder=False, **kwargs)
```

자세한 하이퍼파라미터를 알기 위해서는 다음을 확인하십시오.

[Python API Reference &mdash; xgboost 1.6.1 documentation](https://xgboost.readthedocs.io/en/stable/python/python_api.html)

**Light GBM**은 Level-wise 트리분할 대신 Leaf-wise 트리분할 방식으로 비대칭적인 트리를 생성합니다. Light GBM을 사용하기 위해서는 `lightgbm`라이브러리를 임포트합니다. 

```python
class lightgbm.LGBMClassifier(boosting_type='gbdt', num_leaves=31, 
max_depth=- 1, learning_rate=0.1, n_estimators=100, 
subsample_for_bin=200000, objective=None, class_weight=None, 
min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, 
subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, 
reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', 
**kwargs)
```

```python
class lightgbm.LGBMRegressor(boosting_type='gbdt', num_leaves=31, 
max_depth=- 1, learning_rate=0.1, n_estimators=100, 
subsample_for_bin=200000, objective=None, class_weight=None, 
min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, 
subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, 
reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', 
**kwargs)
```

자세한 하이퍼파라미터에 대해서는 다음을 확인하십시오.

[Python API &mdash; LightGBM 3.3.2.99 documentation](https://lightgbm.readthedocs.io/en/latest/Python-API.html)
